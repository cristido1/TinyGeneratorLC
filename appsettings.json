{
  "LlamaCpp": {
    "Root": "C:\\llama.cpp.13.1",
    "Models": "C:\\llama.cpp.13.1\\models",
    "ServerExe": "C:\\llama.cpp.13.1\\llama-server.exe",
    "Host": "127.0.0.1",
    "Port": 11436,
    "RestartPolicy": "kill",
    "GpuLayers": 0,
    "Device": "CUDA0",
    "CudaBinPath": "C:\\llama.cpp.13.1;C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.4\\bin"
  },
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*"
  ,
  "AudioCraft": {
    "ForceCpu": false
  }
  ,
  "Debug": {
    "EnableOutboundGeneration": false
  },
  "OpenAI": {
    "endpoint": "https://api.openai.com/v1/responses"
  },
  "Ollama": {
    "endpoint": "http://localhost:11434"
  },
  "Memory": {
    "Embeddings": {
      "Model": "nomic-embed-text:latest",
      "BackfillBatchSize": 4,
      "RequestTimeoutSeconds": 120,
      "EmbeddingDimension": 768,
      "SqliteVecExtensionPath": ""
    }
  },
  "AppLog": {
    "LogRequestResponse": true,
    "OtherLogs": true,
    "LogToolResponses": false
  },
  "CommandDispatcher": {
    "MaxParallelCommands": 1
  },
  "NarratorVoice": {
    "DefaultVoiceId": "Dionisio Schuyler"
  }
}
